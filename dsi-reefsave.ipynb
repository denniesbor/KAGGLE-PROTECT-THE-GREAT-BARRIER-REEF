{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# <center>[Tensorflow - Help Protect the Great Barrier Reef](https://www.kaggle.com/c/tensorflow-great-barrier-reef)</center>\n> <center>Detecting crown-of-thorns starfish in underwater image data</center>\n\n<center><img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/31703/logos/header.png?t=2021-10-29-00-30-04\" ></center>\n\n<center><h1>Report: Team ReefSave</h1></center>\n\n<h1>Introduction</h1>\n    \nüê† This notebook is submitted to the Kaggle competition: TensorFlow ‚Äì Help Protect the Great Barrier Reef that run between November 22, 2021 and February 14, 2022. \n\nüê† The goal of the competition is to accurately identify starfish in real-time by building an object detection model trained on underwater videos of coral reefs.\n\nüê† This will help researchers identify species that are threatening Australia's Great Barrier Reef and take well-informed action to protect the reef for future generations.\n\nüê† Additional information covering Description, Evaluation, Timeline, Prizes and Code Requirements can be found at https://www.kaggle.com/c/tensorflow-great-barrier-reef/overview\n\nüê† This notebook is also submitted as report for the first module project of the [Africa DSI program 2022](http://dsi-program.com/).","metadata":{}},{"cell_type":"markdown","source":"<h1>Artificial Learning (AI), Machine Learning (ML) and Deep Learning (DL)</h1>\n\nThe approach used for this project is YOLO, an algorithm that employs convolutional neural networks (CNN) to detect objects in real-time.\n\nCNN is a type of Deep Learning (DL) algorithm most commonly used to analyze visual imagery.\n\n*Source: [Introduction to Convolutional Neural Networks (CNN)](https://www.analyticsvidhya.com/blog/2021/05/convolutional-neural-networks-cnn/#:~:text=In%20deep%20learning%2C%20a%20convolutional,applied%20to%20analyze%20visual%20imagery.&text=It%20uses%20a%20special%20technique%20called%20Convolution/)*.\n\n<div align=\"center\"><img src=\"https://flatironschool.com/legacy-assets/images.ctfassets.net/hkpf2qd2vxgx/235ViW0mhGaFw3bjXUrUyG/35d7a4312bb78fc47a644877ac01c6ea/BlogGraphics-machinnelearning-dark-09__1_.png\" width=700>\n\n<center><h5>Relationship between Artificial Intelligence, Machine Learning and Deep Learning</h5></center>","metadata":{}},{"cell_type":"markdown","source":"## What is CNN?\n‚Äã\nüê† A Convolutional Neural Network (ConvNet/CNN) works by assigning importance (learnable weights and biases) to various aspects/objects of an image with the ability to differentiate one from the other.\nThe pre-processing required in a ConvNet is much lower as compared to other classification algorithms. \n\nüê† CNN is analogous to the connectivity pattern of neurons in the human brain and was inspired by the organization of the Visual Cortex. Individual neurons respond to stimuli only in a restricted region of the visual field known as the Receptive Field. A collection of such fields overlap to cover the entire visual area.\n\nüê† CNN uses multiple layers of artificial neurons or mathematical functions that calculate the weighted sum of multiple inputs and outputs an activation value. Each layer generates several activation functions that are passed on to the next layer.\n\nüê† The first layer usually extracts basic features such as horizontal or diagonal edges.\nThis output is passed on to the next layer which detects more complex features such as corners or combinational edges. As we move deeper into the network it can identify even more complex features such as objects, faces, etc. Based on the activation map of the final convolution layer, the classification layer outputs a set of confidence scores (values between 0 and 1) that specify how likely the image is to belong to a ‚Äúclass‚Äù.\n\n<div align=\"center\"><img src=\"https://miro.medium.com/max/1400/1*Xn14QMJ7pzusY68MW9m8pQ.png\n\" width=400>\n‚Äã\n#  \n<div align=\"center\"><img src=\"https://miro.medium.com/max/700/1*qtinjiZct2w7Dr4XoFixnA.gif\" width=400>\n    \n<center><h5>How CNN works</h5></center>\n\nReferences: \n\n*[A Comprehensive Guide to Convolutional Neural Networks ‚Äî the ELI5 way](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53#:~:text=A%20Convolutional%20Neural%20Network%20(ConvNet,differentiate%20one%20from%20the%20other.)*\n\n*[Introduction to Convolutional Neural Networks (CNN)](https://www.analyticsvidhya.com/blog/2021/05/convolutional-neural-networks-cnn/#:~:text=Convolutional%20neural%20networks%20are%20composed,and%20outputs%20an%20activation%20value.)*\n\n*[Convolutional Neural Networks](https://www.sciencedirect.com/topics/engineering/convolutional-neural-networks)*","metadata":{}},{"cell_type":"markdown","source":"## YOLO\n\nüê† **YOLO**: The acronym stands for ‚ÄòYou Only Look Once‚Äô, a reference to the fact that the algorithm requires only a single forward pass through a neural network to identify objects\n\nüê† **What is it?** YOLO it's a very simple and fast algorithm that recognizes objects within an image in real time. It is made up by a single CNN and requires only one forward pass through the neural network in order to identify the objects.\n\nüê† **How does it work?**\n\n1. The image is split in a grid that has the same dimension for each \"tile\".\n2. Bounding boxes that identify each object are added. The bbox has the following format: `[width, height, class, bx, by]`, where `[bx, by]` represents the center of the object.\n3. Intersection Over Union: this technique is used so the bounding box \"catches\" the object fully (and doesn't leave any part of it uncovered, neither it is too large for the object). The `IOU=1` if the predicted and actual box are identical.\n\n<center><img src=\"https://www.section.io/engineering-education/introduction-to-yolo-algorithm-for-object-detection/bounding-box.png\" width=600 height=300></center>\n\n\n<center><img src=\"https://i.imgur.com/Ce1sfqj.png\" width=600></center>\n\n    \nüê† **Why YOLO?** \n    \nYOLO has gained popularity in computer vision for the following reasons:\n1. Speed: Currently, YOLO is typically faster than alternative algorithms\n\n2. High accuracy: YOLO as a predictive technique, provides accurate results with minimal background errors.\n\n3. Learning capabilities: The algorithm has excellent learning capabilities that enable it to learn the representations of objects and apply them in object detection.\n\nA comparison between YOLOv5, ResNet and Faster RCNN using the competition project data and base parameters showed that YOLOv5 performs better in both speed and mean average precision (MAP). The results can be found in [here](https://github.com/denniesbor/KAGGLE-PROTECT-THE-GREAT-BARRIER-REEF)\n\nüê† **Implementing YOLO?** \n    \nThe version used for this project is YOLOv5. This requires cloning the official repository and setting up the dependencies required to run YOLO v5. \n\nYOLOv5 repository: https://github.com/ultralytics/yolov5\n\nThe steps for implementing YOLOv5 are:\n\n1. Set up the Code\n2. Download the Data\n3. Convert the Annotations into the YOLO v5 Format\n >YOLO v5 Annotation Format\n \n >Testing the annotations\n \n >Partition the Dataset\n4. Training Options\n >Data Config File\n \n >Hyper-parameter Config File\n \n >Custom Network Architecture\n \n >Train the Model\n5. Inference\n >Computing the mAP on test dataset\n6. Conclusion... and a bit about the naming saga\n\n\nüê† **Data Format**\n> 3 main inputs are necessary for YOLOv5\n1. Set of training images\n\n2. Annotation files in .txt format. Each .txt file contains the annotations for the corresponding image file, that is object class, object coordinates, height and width.\n\n3. YAML file containing model configuration and class values.\n\n\n*References*\n\n*[How to Train YOLO v5 on a Custom Dataset](https://blog.paperspace.com/train-yolov5-custom-data/)*\n\n*[Deep Learning vs. Machine Learning ‚Äî What‚Äôs the Difference?](https://flatironschool.com/blog/deep-learning-vs-machine-learning/)*\n\n*[Introduction to YOLO Algorithm for Object Detection](https://www.section.io/engineering-education/introduction-to-yolo-algorithm-for-object-detection/)*","metadata":{}},{"cell_type":"markdown","source":"# Literature\n\nThe choice of a neural network is dependent on the available software and hardware resources, speed ,and the expected accuracy. Object detection networks are classified as multi-stage or single stage. \n\nExamples of single staged neural nets are the SSD, YOLO, etc. The multi staged approaches uses the region proposal networks in their architectures to extract feature maps from the backbone. Examples of multi stage networks are the RCNN and RFCN.\n\n<h3> <strong>Architecture of a neural network</strong></h3>\n\nObject detection nets consists of the input, backbone, neck and the head. The input takes in an image, and it outputs to a feature extractor consisting of dense convolution and max pooling layers. Residual Network(ResNet), ResNext,DenseNet, VGG16 etc. are the commonly used backbones. They are trained on standardized datasets such as [COCO](https://cocodataset.org/#home) or [ImageNet](https://image-net.org).\n<br/>\nThe role of the neck is to extract feature maps e.g the Feature Pyramid Network. The head of a single stage network is dense prediction layer and sparse prediction for a two stage detector(i.e RCNN & RFCN)\n<br />\n![](https://github.com/denniesbor/KAGGLE-PROTECT-THE-GREAT-BARRIER-REEF/blob/e0c3e4253d8fd6a867252cab1feb3bab3d80f377/object_detection_arch.png?raw=true)\n<br />\n[**Figure 1.** Schematic representation of a single and multi stage neural network. Source: [Ultralytics](https://arxiv.org/pdf/1611.10012.pdf) ]\n\n<h3> <strong>The choice of a neural network.</strong></h3>\n\nComputational resources determine the amount of time spent on training and inference. GPU and TPU runtime accelerate the training as well as the inference time. The computational resource demand differ from one model to another.\n\nSpeed is key in a real-time object detection system or video search engines.  A balance of speed and resource requirements  is considered to achieve optimal performance.\n\nThe implementation of the minimum viable product for the module one was based on the performance of the Faster R-CNN ResNet Inception, Yolov4 and Yolov5 on pre-processed TensorFlow-Protect the great barrier datasets.\n\n<h3><strong>Yolo(Single stage)</strong></h3>\n\nYolo is a single stage state of the art object detection algorithm. There are 4 documented versions of YOLO and the fifth version designed by Ultralytics team. [YOLO](https://github.com/ultralytics/yolov5) is described as a YOLOv4 implementation in Pytorch.\nCompared with other algorithms, YOLO5 perfoms exceptionally well with a less GPU time.\n\nAccording to [Huang,et al](https://arxiv.org/pdf/1611.10012.pdf) YOLO v4 attains a mean average precision of 43.5 running on a Tesla V100 GPUs while training on Common Objects in Context datasets. The neck of YOLO4 uses SPP and PAN.\n<br />\n![yolo](https://github.com/denniesbor/KAGGLE-PROTECT-THE-GREAT-BARRIER-REEF/blob/assets/Yolov5_performance.png?raw=true)\n<br/>\n[**Figure 2.** Average precision vs GPU speed of *YOLO5* weights against *EfficientDet* on . on [COCO](https://cocodataset.org/#home) datasets. Source: [Ultralytics](https://github.com/ultralytics/yolov5) ]\n\n### What are Bag of Freebies and Bag of Specials?\n\nThey define the inference - training trade-off of a model. The bag of freebies are the methods applied to the model and which does not interfere with inference. Some of these methods include the data augmentation, regularization techniques e.g., dropout, drop-connect and drop-block.\n\nThe bag of freebies are the methods which improve the accuracy of the model by at the expense of inference costs. These methods introduce attention mechanisms. SPP is an example of this feature and is applied in YOLOv4.\n\n<h3> <strong>Faster R-CNN(Multi stage)</strong></h3>\n\nR-CNN models is a multi layered conv neural network and consists of the feature extractor, a region proposal algorithm to generate bounding boxes, a regression and classification layer. R-CNNs tradeoff their speed for accuracy. \n\nIn Faster R-CNN, Region Proposal Network generation is not CPU restricted compared to the previous flavours of region convolution neural network.\n<br />\n![](https://github.com/denniesbor/KAGGLE-PROTECT-THE-GREAT-BARRIER-REEF/blob/assets/feature_extractor_acc.png?raw=True)\n<br />\n[**Figure 3.** Mean average precision against backbone accuracy of Faster R-CNN, R-FCN and SSD]\n<h2> <strong>MVP - Performance comparison of YOLOv4, YOLOv5 and R-CNN</strong></h2>\nThe TensorFlow- Save the Great Barrier mvp is implemented using Faster R-CNN, YOLO4 and YOLO5 default tuning parameters. Performance analysis of the three models is done using their mean average precision. Faster RCNN runs on Resnet Inception backbone, whereas YOLO4 is built on darknet.\n\n","metadata":{"id":"uaqdfXy2tsWu"}},{"cell_type":"markdown","source":"\n\n# Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"markdown","source":"Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport cv2\nimport os\nimport seaborn as sns\nimport ast\nimport time","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:23.765800Z","iopub.execute_input":"2022-02-09T12:46:23.766201Z","iopub.status.idle":"2022-02-09T12:46:23.772046Z","shell.execute_reply.started":"2022-02-09T12:46:23.766164Z","shell.execute_reply":"2022-02-09T12:46:23.771051Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Check Directory","metadata":{}},{"cell_type":"code","source":"!ls ../input/tensorflow-great-barrier-reef","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:23.778953Z","iopub.execute_input":"2022-02-09T12:46:23.780209Z","iopub.status.idle":"2022-02-09T12:46:24.628499Z","shell.execute_reply.started":"2022-02-09T12:46:23.780157Z","shell.execute_reply":"2022-02-09T12:46:24.627077Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Load Data to dataframe from csv","metadata":{}},{"cell_type":"code","source":"\ndf = pd.read_csv('../input/tensorflow-great-barrier-reef/train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:24.630744Z","iopub.execute_input":"2022-02-09T12:46:24.631023Z","iopub.status.idle":"2022-02-09T12:46:24.679201Z","shell.execute_reply.started":"2022-02-09T12:46:24.630995Z","shell.execute_reply":"2022-02-09T12:46:24.678126Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**Explore data**\nEDA from https://www.kaggle.com/kartik2khandelwal/data-analysis-and-prediction/notebook\n\nCheck shapes for column and rows, and data types of columns","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:24.680642Z","iopub.execute_input":"2022-02-09T12:46:24.680872Z","iopub.status.idle":"2022-02-09T12:46:24.725573Z","shell.execute_reply.started":"2022-02-09T12:46:24.680845Z","shell.execute_reply":"2022-02-09T12:46:24.724574Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:24.727589Z","iopub.execute_input":"2022-02-09T12:46:24.728180Z","iopub.status.idle":"2022-02-09T12:46:24.751582Z","shell.execute_reply.started":"2022-02-09T12:46:24.728145Z","shell.execute_reply":"2022-02-09T12:46:24.750464Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:24.753497Z","iopub.execute_input":"2022-02-09T12:46:24.753770Z","iopub.status.idle":"2022-02-09T12:46:24.759026Z","shell.execute_reply.started":"2022-02-09T12:46:24.753740Z","shell.execute_reply":"2022-02-09T12:46:24.758180Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(df.dtypes)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:24.760632Z","iopub.execute_input":"2022-02-09T12:46:24.761158Z","iopub.status.idle":"2022-02-09T12:46:24.775619Z","shell.execute_reply.started":"2022-02-09T12:46:24.761122Z","shell.execute_reply":"2022-02-09T12:46:24.774650Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Check for null values\n - No null value found","metadata":{}},{"cell_type":"code","source":"null_count = df.isnull().sum()\nprint(null_count)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:24.776974Z","iopub.execute_input":"2022-02-09T12:46:24.777426Z","iopub.status.idle":"2022-02-09T12:46:24.795902Z","shell.execute_reply.started":"2022-02-09T12:46:24.777393Z","shell.execute_reply":"2022-02-09T12:46:24.795085Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Count number of unique images\n - 23501 unique images","metadata":{}},{"cell_type":"code","source":"df.image_id.count()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:24.797113Z","iopub.execute_input":"2022-02-09T12:46:24.797824Z","iopub.status.idle":"2022-02-09T12:46:24.813032Z","shell.execute_reply.started":"2022-02-09T12:46:24.797775Z","shell.execute_reply":"2022-02-09T12:46:24.811408Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Number of frames in one video\n- 10688 frames per video","metadata":{}},{"cell_type":"code","source":"df.video_frame.unique()\nprint(len(pd.unique(df['video_frame'])))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:24.814923Z","iopub.execute_input":"2022-02-09T12:46:24.815582Z","iopub.status.idle":"2022-02-09T12:46:24.828649Z","shell.execute_reply.started":"2022-02-09T12:46:24.815480Z","shell.execute_reply":"2022-02-09T12:46:24.827706Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Maximum frame value in any of the videos","metadata":{}},{"cell_type":"code","source":"df.video_frame.max()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:24.831444Z","iopub.execute_input":"2022-02-09T12:46:24.831801Z","iopub.status.idle":"2022-02-09T12:46:24.842878Z","shell.execute_reply.started":"2022-02-09T12:46:24.831765Z","shell.execute_reply":"2022-02-09T12:46:24.842203Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"df['image.path'] = os.path.join('../input/tensorflow-great-barrier-reef/train.csv')+\"/video_\"+df.video_id.astype(str)+\"/\"+df.video_frame.astype(str)+\".jpg\"","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:24.843953Z","iopub.execute_input":"2022-02-09T12:46:24.844204Z","iopub.status.idle":"2022-02-09T12:46:24.908357Z","shell.execute_reply.started":"2022-02-09T12:46:24.844176Z","shell.execute_reply":"2022-02-09T12:46:24.907248Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:24.912174Z","iopub.execute_input":"2022-02-09T12:46:24.912659Z","iopub.status.idle":"2022-02-09T12:46:24.925870Z","shell.execute_reply.started":"2022-02-09T12:46:24.912623Z","shell.execute_reply":"2022-02-09T12:46:24.924603Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Check number of images in each video","metadata":{}},{"cell_type":"code","source":"list(df['video_id'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:24.927616Z","iopub.execute_input":"2022-02-09T12:46:24.929386Z","iopub.status.idle":"2022-02-09T12:46:24.945538Z","shell.execute_reply.started":"2022-02-09T12:46:24.929147Z","shell.execute_reply":"2022-02-09T12:46:24.944529Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Plot of number of images in each video","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.countplot(df['video_id'], color='#2196F3')","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:24.947279Z","iopub.execute_input":"2022-02-09T12:46:24.948252Z","iopub.status.idle":"2022-02-09T12:46:25.461888Z","shell.execute_reply.started":"2022-02-09T12:46:24.947820Z","shell.execute_reply":"2022-02-09T12:46:25.460771Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Check how many images have labels (bounding boxes) and whether all contain \"[]\"","metadata":{}},{"cell_type":"code","source":"with_annotation = len(df[df['annotations'] != '[]'])\nwithout_annotation = len(df[df['annotations'] == '[]'])","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:25.463528Z","iopub.execute_input":"2022-02-09T12:46:25.463860Z","iopub.status.idle":"2022-02-09T12:46:25.483040Z","shell.execute_reply.started":"2022-02-09T12:46:25.463814Z","shell.execute_reply":"2022-02-09T12:46:25.481441Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Check images with annotations\nVerify that all annotations are bounded by []","metadata":{}},{"cell_type":"code","source":"#\nannotated=df[df['annotations'] != '[]']\n# see format of annotations\nprint(annotated['annotations'])\nS=pd.Series(annotated['annotations'])\n\nprint(S.str.count(r'(!^\\[.*])').sum())\nprint(S.str.count(r'(^\\[.*])').sum())\n","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:25.485767Z","iopub.execute_input":"2022-02-09T12:46:25.486351Z","iopub.status.idle":"2022-02-09T12:46:25.516728Z","shell.execute_reply.started":"2022-02-09T12:46:25.486282Z","shell.execute_reply":"2022-02-09T12:46:25.515580Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Check if data is balanced\n- Data is inbalanced","metadata":{}},{"cell_type":"code","source":"\nlabels = ('Without Bounding Box', 'With Bounding Box')\ny_pos = np.arange(len(labels))\ncount_annotations = [without_annotation, with_annotation]\n\nplt.bar(y_pos, count_annotations, align='center', color='#2196F3', alpha=1.0)\nplt.xticks(y_pos, labels)\nplt.ylabel('Count')\nplt.title('labels')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:25.518358Z","iopub.execute_input":"2022-02-09T12:46:25.519340Z","iopub.status.idle":"2022-02-09T12:46:25.712241Z","shell.execute_reply.started":"2022-02-09T12:46:25.519281Z","shell.execute_reply":"2022-02-09T12:46:25.711152Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"There can be more than one annotation in one frame\n\nFirst test the format of annotations","metadata":{}},{"cell_type":"code","source":"print(len(df['annotations']))\nprint(len(df[df['annotations'].apply(lambda x:len(str(x))) > 2]))\nprint(len(df[df['annotations'].apply(lambda x:len(str(x))) > 50]))\nprint(df[df['annotations'].apply(lambda x:len(str(x))) > 500]['annotations'].iloc[0])\n","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:25.713514Z","iopub.execute_input":"2022-02-09T12:46:25.714139Z","iopub.status.idle":"2022-02-09T12:46:25.768598Z","shell.execute_reply.started":"2022-02-09T12:46:25.714088Z","shell.execute_reply":"2022-02-09T12:46:25.767807Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"Count number of annotations per image","metadata":{}},{"cell_type":"code","source":"df['sum_annotations'] = df['annotations'].apply(lambda x: x.count('{'))\n#print(df['annotations'][9292])\nprint(df[df['sum_annotations']>11])\n","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:25.769703Z","iopub.execute_input":"2022-02-09T12:46:25.770689Z","iopub.status.idle":"2022-02-09T12:46:25.798482Z","shell.execute_reply.started":"2022-02-09T12:46:25.770645Z","shell.execute_reply":"2022-02-09T12:46:25.797690Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"Plot of number of annotations per image\n- Range between 1 and 17\n- Gives indication of outliers\n- Over 50% of images with annotations have only 1 annotation","metadata":{}},{"cell_type":"code","source":"fig = px.bar(df['sum_annotations'].value_counts().drop(0), title='Count of Bounding Boxes', width=700)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:25.799601Z","iopub.execute_input":"2022-02-09T12:46:25.800478Z","iopub.status.idle":"2022-02-09T12:46:27.481714Z","shell.execute_reply.started":"2022-02-09T12:46:25.800432Z","shell.execute_reply":"2022-02-09T12:46:27.480826Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Model Training\n","metadata":{}},{"cell_type":"markdown","source":"üê† Model training started on kaggle platform \n\nüê† Optimized on AWS\n\nüê† The notebook for the model training is located at\n\nüê† Note that directories to data are different\n\nüê† The code for the model training is below","metadata":{}},{"cell_type":"markdown","source":"# üõ† Install Libraries","metadata":{}},{"cell_type":"code","source":"!pip install -qU wandb\n!pip install -qU bbox-utility # check https://github.com/awsaf49/bbox for source code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:46:27.483346Z","iopub.execute_input":"2022-02-09T12:46:27.483849Z","iopub.status.idle":"2022-02-09T12:46:52.900539Z","shell.execute_reply.started":"2022-02-09T12:46:27.483805Z","shell.execute_reply":"2022-02-09T12:46:52.899638Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# üìö Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport glob\n\nimport shutil\nimport sys\nsys.path.append('../input/tensorflow-great-barrier-reef')\n\nfrom joblib import Parallel, delayed\n\nfrom IPython.display import display","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:46:52.902766Z","iopub.execute_input":"2022-02-09T12:46:52.903406Z","iopub.status.idle":"2022-02-09T12:46:53.003495Z","shell.execute_reply.started":"2022-02-09T12:46:52.903353Z","shell.execute_reply":"2022-02-09T12:46:53.002473Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# üìå Key-Points\n* One have to submit prediction using the provided **python time-series API**, which makes this competition different from previous Object Detection Competitions.\n* Each prediction row needs to include all bounding boxes for the image. Submission is format seems also **COCO** which means `[x_min, y_min, width, height]`\n* Copmetition metric `F2` tolerates some false positives(FP) in order to ensure very few starfish are missed. Which means tackling **false negatives(FN)** is more important than false positives(FP). \n$$F2 = 5 \\cdot \\frac{precision \\cdot recall}{4\\cdot precision + recall}$$","metadata":{}},{"cell_type":"markdown","source":"# ‚≠ê WandB\n<img src=\"https://camo.githubusercontent.com/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\" width=600>\n\nWeights & Biases (W&B) is MLOps platform for tracking our experiemnts. We can use it to Build better models faster with experiment tracking, dataset versioning, and model management. Some of the cool features of W&B:\n\n* Track, compare, and visualize ML experiments\n* Get live metrics, terminal logs, and system stats streamed to the centralized dashboard.\n* Explain how your model works, show graphs of how model versions improved, discuss bugs, and demonstrate progress towards milestones.\n","metadata":{}},{"cell_type":"code","source":"import wandb\n#%env WANDB_NOTEBOOK_NAME 'DSI_reef/Nmeso/Nmeso_great-barrier-reef-yolov5-train.ipynb'\n#%env WANDB_API_KEY=\"2bee544001fe452e298497e9e54c59a76ebe1563\"\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"WANDB\")\n    wandb.login(key=api_key)\n    anonymous = None\nexcept:\n    wandb.login(anonymous='must')\n    print('To use your W&B account,\\nGo to Add-ons -> Secrets and provide your W&B access token. Use the Label name as WANDB. \\nGet your W&B access token from here: https://wandb.ai/authorize')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:46:53.004939Z","iopub.execute_input":"2022-02-09T12:46:53.005232Z","iopub.status.idle":"2022-02-09T12:46:55.461722Z","shell.execute_reply.started":"2022-02-09T12:46:53.005200Z","shell.execute_reply":"2022-02-09T12:46:55.460129Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# üìñ Meta Data\n* `train_images/` - Folder containing training set photos of the form `video_{video_id}/{video_frame}.jpg`.\n\n* `[train/test].csv` - Metadata for the images. As with other test files, most of the test metadata data is only available to your notebook upon submission. Just the first few rows available for download.\n\n* `video_id` - ID number of the video the image was part of. The video ids are not meaningfully ordered.\n* `video_frame` - The frame number of the image within the video. Expect to see occasional gaps in the frame number from when the diver surfaced.\n* `sequence` - ID of a gap-free subset of a given video. The sequence ids are not meaningfully ordered.\n* `sequence_frame` - The frame number within a given sequence.\n* `image_id` - ID code for the image, in the format `{video_id}-{video_frame}`\n* `annotations` - The bounding boxes of any starfish detections in a string format that can be evaluated directly with Python. Does not use the same format as the predictions you will submit. Not available in test.csv. A bounding box is described by the pixel coordinate `(x_min, y_min)` of its lower left corner within the image together with its `width` and `height` in pixels --> (COCO format).","metadata":{}},{"cell_type":"code","source":"FOLD      = 1 # which fold to train\nDIM       = 700 \nMODEL     = 'yolov5s6'\nBATCH     = 4\nEPOCHS    = 30\nOPTMIZER  = 'Adam'\n\nPROJECT   = 'great-barrier-reef-public' # w&b in yolov5\nNAME      = f'{MODEL}-dim{DIM}-fold{FOLD}' # w&b for yolov5\n\nREMOVE_NOBBOX = True # remove images with no bbox\nWORKING = '/kaggle/working'\nROOT_DIR  = '../input/tensorflow-great-barrier-reef'\nIMAGE_DIR = '/kaggle/working/images' # directory to save images\nLABEL_DIR = '/kaggle/working/labels' # directory to save labels","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:46:55.465884Z","iopub.execute_input":"2022-02-09T12:46:55.466335Z","iopub.status.idle":"2022-02-09T12:46:55.472754Z","shell.execute_reply.started":"2022-02-09T12:46:55.466285Z","shell.execute_reply":"2022-02-09T12:46:55.471711Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Create Directories","metadata":{}},{"cell_type":"code","source":"!mkdir -p {IMAGE_DIR}\n!mkdir -p {LABEL_DIR}\n!mkdir -p {WORKING}","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:46:55.474620Z","iopub.execute_input":"2022-02-09T12:46:55.475636Z","iopub.status.idle":"2022-02-09T12:46:57.956619Z","shell.execute_reply.started":"2022-02-09T12:46:55.475580Z","shell.execute_reply":"2022-02-09T12:46:57.955542Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"!ls ../input/tensorflow-great-barrier-reef\n#!ls {ROOT_DIR}","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:57.958146Z","iopub.execute_input":"2022-02-09T12:46:57.958590Z","iopub.status.idle":"2022-02-09T12:46:58.763334Z","shell.execute_reply.started":"2022-02-09T12:46:57.958558Z","shell.execute_reply":"2022-02-09T12:46:58.762291Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#!cp -pr ../input/tensorflow-great-barrier-reef /kaggle/working/\n#!ls /kaggle/working/tensorflow-great-barrier-reef","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:46:58.766401Z","iopub.execute_input":"2022-02-09T12:46:58.767302Z","iopub.status.idle":"2022-02-09T12:46:58.772320Z","shell.execute_reply.started":"2022-02-09T12:46:58.767254Z","shell.execute_reply":"2022-02-09T12:46:58.770932Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Get Paths","metadata":{}},{"cell_type":"code","source":"# Train Data\ndf = pd.read_csv(f'{ROOT_DIR}/train.csv')\ndf['old_image_path'] = f'{ROOT_DIR}/train_images/video_'+df.video_id.astype(str)+'/'+df.video_frame.astype(str)+'.jpg'\ndf['image_path']  = f'{IMAGE_DIR}/'+df.image_id+'.jpg'\ndf['label_path']  = f'{LABEL_DIR}/'+df.image_id+'.txt'\ndf['annotations'] = df['annotations'].progress_apply(eval)\ndisplay(df.head(2))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:46:58.778303Z","iopub.execute_input":"2022-02-09T12:46:58.779073Z","iopub.status.idle":"2022-02-09T12:46:59.359802Z","shell.execute_reply.started":"2022-02-09T12:46:58.779033Z","shell.execute_reply":"2022-02-09T12:46:59.358343Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## Number of BBoxes\n> Nearly 80% images are without any bbox.","metadata":{}},{"cell_type":"code","source":"df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\ndata = (df.num_bbox>0).value_counts(normalize=True)*100\nprint(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:46:59.361331Z","iopub.execute_input":"2022-02-09T12:46:59.361592Z","iopub.status.idle":"2022-02-09T12:46:59.444244Z","shell.execute_reply.started":"2022-02-09T12:46:59.361564Z","shell.execute_reply":"2022-02-09T12:46:59.443206Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# üßπ Clean Data\n* In this notebook, we use only **bboxed-images** (`~5k`). We can use all `~23K` images for train but most of them don't have any labels. So it would be easier to carry out experiments using only **bboxed images**.","metadata":{}},{"cell_type":"code","source":"if REMOVE_NOBBOX:\n    df = df.query(\"num_bbox>0\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:46:59.445713Z","iopub.execute_input":"2022-02-09T12:46:59.446049Z","iopub.status.idle":"2022-02-09T12:46:59.468640Z","shell.execute_reply.started":"2022-02-09T12:46:59.446006Z","shell.execute_reply":"2022-02-09T12:46:59.467417Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# ‚úèÔ∏è Write Images\n* We need to copy the Images to Current Directory(`/kaggle/working`) as `/kaggle/input` doesn't have **write access** which is needed for **YOLOv5**.\n* We can make this process faster using **Joblib** which uses **Parallel** computing.","metadata":{}},{"cell_type":"code","source":"def make_copy(row):\n    shutil.copyfile(row.old_image_path, row.image_path)\n    return","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:46:59.470191Z","iopub.execute_input":"2022-02-09T12:46:59.470558Z","iopub.status.idle":"2022-02-09T12:46:59.475951Z","shell.execute_reply.started":"2022-02-09T12:46:59.470512Z","shell.execute_reply":"2022-02-09T12:46:59.474655Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"image_paths = df.old_image_path.tolist()\n_ = Parallel(n_jobs=-1, backend='threading')(delayed(make_copy)(row) for _, row in tqdm(df.iterrows(), total=len(df)))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:46:59.477470Z","iopub.execute_input":"2022-02-09T12:46:59.478121Z","iopub.status.idle":"2022-02-09T12:47:18.812454Z","shell.execute_reply.started":"2022-02-09T12:46:59.477758Z","shell.execute_reply":"2022-02-09T12:47:18.811690Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# check https://github.com/awsaf49/bbox for source code of following utility functions\nfrom bbox.utils import coco2yolo, coco2voc, voc2yolo\nfrom bbox.utils import draw_bboxes, load_image\nfrom bbox.utils import clip_bbox, str2annot, annot2str\n\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_imgsize(row):\n    row['width'], row['height'] = imagesize.get(row['image_path'])\n    return row\n\nnp.random.seed(32)\ncolors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n          for idx in range(1)]","metadata":{"_kg_hide-input":false,"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:47:18.813497Z","iopub.execute_input":"2022-02-09T12:47:18.813738Z","iopub.status.idle":"2022-02-09T12:47:19.756865Z","shell.execute_reply.started":"2022-02-09T12:47:18.813709Z","shell.execute_reply":"2022-02-09T12:47:19.755445Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"## Create BBox","metadata":{}},{"cell_type":"code","source":"df['bboxes'] = df.annotations.progress_apply(get_bbox)\ndf.head(2)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:47:19.758629Z","iopub.execute_input":"2022-02-09T12:47:19.758906Z","iopub.status.idle":"2022-02-09T12:47:19.837498Z","shell.execute_reply.started":"2022-02-09T12:47:19.758875Z","shell.execute_reply":"2022-02-09T12:47:19.836541Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## Get Image-Size\n> All Images have same dimension, [Width, Height] =  `[1280, 720]`","metadata":{}},{"cell_type":"code","source":"df['width']  = 1280\ndf['height'] = 720\ndisplay(df.head(2))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:47:19.838797Z","iopub.execute_input":"2022-02-09T12:47:19.839111Z","iopub.status.idle":"2022-02-09T12:47:19.860204Z","shell.execute_reply.started":"2022-02-09T12:47:19.839080Z","shell.execute_reply":"2022-02-09T12:47:19.859503Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"# üè∑Ô∏è Create Labels\nWe need to export our labels to **YOLO** format, with one `*.txt` file per image (if no objects in image, no `*.txt` file is required). The *.txt file specifications are:\n\n* One row per object\n* Each row is class `[x_center, y_center, width, height]` format.\n* Box coordinates must be in **normalized** `xywh` format (from `0 - 1`). If your boxes are in pixels, divide `x_center` and `width` by `image width`, and `y_center` and `height` by `image height`.\n* Class numbers are **zero-indexed** (start from `0`).\n\n> Competition bbox format is **COCO** hence `[x_min, y_min, width, height]`. So, we need to convert form **COCO** to **YOLO** format.\n","metadata":{}},{"cell_type":"code","source":"cnt = 0\nall_bboxes = []\nbboxes_info = []\nfor row_idx in tqdm(range(df.shape[0])):\n    row = df.iloc[row_idx]\n    image_height = row.height\n    image_width  = row.width\n    bboxes_coco  = np.array(row.bboxes).astype(np.float32).copy()\n    num_bbox     = len(bboxes_coco)\n    names        = ['cots']*num_bbox\n    labels       = np.array([0]*num_bbox)[..., None].astype(str)\n    ## Create Annotation(YOLO)\n    with open(row.label_path, 'w') as f:\n        if num_bbox<1:\n            annot = ''\n            f.write(annot)\n            cnt+=1\n            continue\n        bboxes_voc  = coco2voc(bboxes_coco, image_height, image_width)\n        bboxes_voc  = clip_bbox(bboxes_voc, image_height, image_width)\n        bboxes_yolo = voc2yolo(bboxes_voc, image_height, image_width).astype(str)\n        all_bboxes.extend(bboxes_yolo.astype(float))\n        bboxes_info.extend([[row.image_id, row.video_id, row.sequence]]*len(bboxes_yolo))\n        annots = np.concatenate([labels, bboxes_yolo], axis=1)\n        string = annot2str(annots)\n        f.write(string)\nprint('Missing:',cnt)","metadata":{"_kg_hide-input":false,"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:47:19.861368Z","iopub.execute_input":"2022-02-09T12:47:19.861726Z","iopub.status.idle":"2022-02-09T12:47:24.541187Z","shell.execute_reply.started":"2022-02-09T12:47:19.861686Z","shell.execute_reply":"2022-02-09T12:47:24.540005Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# üìÅ Create Folds\n> Number of samples aren't same in each fold which can create large variance in **Cross-Validation**.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\nkf = GroupKFold(n_splits = 3)\ndf = df.reset_index(drop=True)\ndf['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df, groups=df.video_id.tolist())):\n    df.loc[val_idx, 'fold'] = fold\ndisplay(df.fold.value_counts())","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:47:24.543597Z","iopub.execute_input":"2022-02-09T12:47:24.543972Z","iopub.status.idle":"2022-02-09T12:47:24.690867Z","shell.execute_reply.started":"2022-02-09T12:47:24.543927Z","shell.execute_reply":"2022-02-09T12:47:24.690174Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"# ‚≠ï BBox Distribution","metadata":{}},{"cell_type":"code","source":"bbox_df = pd.DataFrame(np.concatenate([bboxes_info, all_bboxes], axis=1),\n             columns=['image_id','video_id','sequence',\n                     'xmid','ymid','w','h'])\nbbox_df[['xmid','ymid','w','h']] = bbox_df[['xmid','ymid','w','h']].astype(float)\nbbox_df['area'] = bbox_df.w * bbox_df.h * 1280 * 720\nbbox_df = bbox_df.merge(df[['image_id','fold']], on='image_id', how='left')\nbbox_df.head(2)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:47:24.691788Z","iopub.execute_input":"2022-02-09T12:47:24.692041Z","iopub.status.idle":"2022-02-09T12:47:24.845956Z","shell.execute_reply.started":"2022-02-09T12:47:24.692004Z","shell.execute_reply":"2022-02-09T12:47:24.845039Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"## `x_center` Vs `y_center`","metadata":{}},{"cell_type":"code","source":"from scipy.stats import gaussian_kde\n\nall_bboxes = np.array(all_bboxes)\n\nx_val = all_bboxes[...,0]\ny_val = all_bboxes[...,1]\n\n# Calculate the point density\nxy = np.vstack([x_val,y_val])\nz = gaussian_kde(xy)(xy)\n\nfig, ax = plt.subplots(figsize = (10, 10))\n# ax.axis('off')\nax.scatter(x_val, y_val, c=z, s=100, cmap='viridis')\n# ax.set_xlabel('x_mid')\n# ax.set_ylabel('y_mid')\nplt.show()","metadata":{"_kg_hide-input":true,"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:47:24.847088Z","iopub.execute_input":"2022-02-09T12:47:24.847320Z","iopub.status.idle":"2022-02-09T12:47:28.107331Z","shell.execute_reply.started":"2022-02-09T12:47:24.847294Z","shell.execute_reply":"2022-02-09T12:47:28.106543Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## `width` Vs `height`","metadata":{}},{"cell_type":"code","source":"x_val = all_bboxes[...,2]\ny_val = all_bboxes[...,3]\n\n# Calculate the point density\nxy = np.vstack([x_val,y_val])\nz = gaussian_kde(xy)(xy)\n\nfig, ax = plt.subplots(figsize = (10, 10))\n# ax.axis('off')\nax.scatter(x_val, y_val, c=z, s=100, cmap='viridis')\n# ax.set_xlabel('bbox_width')\n# ax.set_ylabel('bbox_height')\nplt.show()","metadata":{"_kg_hide-input":true,"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:47:28.108400Z","iopub.execute_input":"2022-02-09T12:47:28.109127Z","iopub.status.idle":"2022-02-09T12:47:30.698757Z","shell.execute_reply.started":"2022-02-09T12:47:28.109092Z","shell.execute_reply":"2022-02-09T12:47:30.697859Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"## Area","metadata":{}},{"cell_type":"code","source":"import matplotlib as mpl\nimport seaborn as sns\n\nf, ax = plt.subplots(figsize=(12, 6))\nsns.despine(f)\n\nsns.histplot(\n    bbox_df,\n    x=\"area\", hue=\"fold\",\n    multiple=\"stack\",\n    palette=\"viridis\",\n    edgecolor=\".3\",\n    linewidth=.5,\n    log_scale=True,\n)\nax.xaxis.set_major_formatter(mpl.ticker.ScalarFormatter())\nax.set_xticks([500, 1000, 2000, 5000, 10000]);","metadata":{"_kg_hide-input":true,"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:47:30.699965Z","iopub.execute_input":"2022-02-09T12:47:30.700226Z","iopub.status.idle":"2022-02-09T12:47:31.731097Z","shell.execute_reply.started":"2022-02-09T12:47:30.700197Z","shell.execute_reply":"2022-02-09T12:47:31.729453Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"# üåà Visualization","metadata":{}},{"cell_type":"code","source":"df2 = df[(df.num_bbox>0)].sample(100) # takes samples with bbox\ny = 3; x = 2\nplt.figure(figsize=(12.8*x, 7.2*y))\nfor idx in range(x*y):\n    row = df2.iloc[idx]\n    img           = load_image(row.image_path)\n    image_height  = row.height\n    image_width   = row.width\n    with open(row.label_path) as f:\n        annot = str2annot(f.read())\n    bboxes_yolo = annot[...,1:]\n    labels      = annot[..., 0].astype(int).tolist()\n    names         = ['cots']*len(bboxes_yolo)\n    plt.subplot(y, x, idx+1)\n    plt.imshow(draw_bboxes(img = img,\n                           bboxes = bboxes_yolo, \n                           classes = names,\n                           class_ids = labels,\n                           class_name = True, \n                           colors = colors, \n                           bbox_format = 'yolo',\n                           line_thickness = 2))\n    plt.axis('OFF')\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:47:31.733082Z","iopub.execute_input":"2022-02-09T12:47:31.733632Z","iopub.status.idle":"2022-02-09T12:47:35.567881Z","shell.execute_reply.started":"2022-02-09T12:47:31.733591Z","shell.execute_reply":"2022-02-09T12:47:35.567091Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"# üçö Dataset","metadata":{}},{"cell_type":"code","source":"train_files = []\nval_files   = []\ntrain_df = df.query(\"fold!=@FOLD\")\nvalid_df = df.query(\"fold==@FOLD\")\ntrain_files += list(train_df.image_path.unique())\nval_files += list(valid_df.image_path.unique())\nlen(train_files), len(val_files)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:48:44.286889Z","iopub.execute_input":"2022-02-09T12:48:44.287694Z","iopub.status.idle":"2022-02-09T12:48:44.309771Z","shell.execute_reply.started":"2022-02-09T12:48:44.287648Z","shell.execute_reply":"2022-02-09T12:48:44.309060Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"# ‚öôÔ∏è Configuration\nThe dataset config file requires\n1. The dataset root directory path and relative paths to `train / val / test` image directories (or *.txt files with image paths)\n2. The number of classes `nc` and \n3. A list of class `names`:`['cots']`","metadata":{}},{"cell_type":"code","source":"import yaml\n\ncwd = '/kaggle/working/'\n\nwith open(os.path.join( cwd , 'train.txt'), 'w') as f:\n    for path in train_df.image_path.tolist():\n        f.write(path+'\\n')\n            \nwith open(os.path.join(cwd , 'val.txt'), 'w') as f:\n    for path in valid_df.image_path.tolist():\n        f.write(path+'\\n')\n\ndata = dict(\n    path  = '/kaggle/working/',\n    train =  os.path.join( cwd , 'train.txt') ,\n    val   =  os.path.join( cwd , 'val.txt' ),\n    nc    = 1,\n    names = ['cots'],\n    )\n\nwith open(os.path.join( cwd , 'gbr.yaml'), 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n\nf = open(os.path.join( cwd , 'gbr.yaml'), 'r')\nprint('\\nyaml:')\nprint(f.read())","metadata":{"_kg_hide-input":true,"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:48:46.094025Z","iopub.execute_input":"2022-02-09T12:48:46.096473Z","iopub.status.idle":"2022-02-09T12:48:46.138470Z","shell.execute_reply.started":"2022-02-09T12:48:46.096391Z","shell.execute_reply":"2022-02-09T12:48:46.133582Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/hyp.yaml\nlr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\nlrf: 0.1  # final OneCycleLR learning rate (lr0 * lrf)\nmomentum: 0.937  # SGD momentum/Adam beta1\nweight_decay: 0.0005  # optimizer weight decay 5e-4\nwarmup_epochs: 2.0  # warmup epochs (fractions ok)\nwarmup_momentum: 0.9  # warmup initial momentum\nwarmup_bias_lr: 0.05  # warmup initial bias lr\nbox: 0.05  # box loss gain\ncls: 0.5  # cls loss gain\ncls_pw: 1.0  # cls BCELoss positive_weight\nobj: 1.0  # obj loss gain (scale with pixels)\nobj_pw: 1.0  # obj BCELoss positive_weight\niou_t: 0.20  # IoU training threshold\nanchor_t: 4.0  # anchor-multiple threshold\nanchors: 3  # anchors per output layer (0 to ignore)\nfl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\nhsv_h: 0.02  # image HSV-Hue augmentation (fraction)\nhsv_s: 0.8  # image HSV-Saturation augmentation (fraction)\nhsv_v: 0.3  # image HSV-Value augmentation (fraction)\ndegrees: 0.0  # image rotation (+/- deg)\ntranslate: 0.10  # image translation (+/- fraction)\nscale: 0.5  # image scale (+/- gain)\nshear: 0.0  # image shear (+/- deg)\nperspective: 0.0  # image perspective (+/- fraction), range 0-0.001\nflipud: 0.5  # image flip up-down (probability)\nfliplr: 0.5  # image flip left-right (probability)\nmosaic: 0.5  # image mosaic (probability)\nmixup: 0.5 # image mixup (probability)\ncopy_paste: 0.0  # segment copy-paste (probability)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-09T12:48:47.605187Z","iopub.execute_input":"2022-02-09T12:48:47.606218Z","iopub.status.idle":"2022-02-09T12:48:47.621573Z","shell.execute_reply.started":"2022-02-09T12:48:47.606150Z","shell.execute_reply":"2022-02-09T12:48:47.619192Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"!pwd","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:48:48.093478Z","iopub.execute_input":"2022-02-09T12:48:48.094163Z","iopub.status.idle":"2022-02-09T12:48:49.006874Z","shell.execute_reply.started":"2022-02-09T12:48:48.094100Z","shell.execute_reply":"2022-02-09T12:48:49.005835Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"sys.path.append('kaggle/working/yolov5')","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:48:49.010187Z","iopub.execute_input":"2022-02-09T12:48:49.011289Z","iopub.status.idle":"2022-02-09T12:48:49.017821Z","shell.execute_reply.started":"2022-02-09T12:48:49.011227Z","shell.execute_reply":"2022-02-09T12:48:49.017026Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/\n# !git clone https://github.com/ultralytics/yolov5 # clone\n!cp -r /kaggle/input/yolov5-lib-ds /kaggle/working/yolov5\n%cd yolov5\n%pip install -qr requirements.txt  # install\n\n\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-02-09T12:48:49.019425Z","iopub.execute_input":"2022-02-09T12:48:49.019972Z","iopub.status.idle":"2022-02-09T12:48:51.824149Z","shell.execute_reply.started":"2022-02-09T12:48:49.019938Z","shell.execute_reply":"2022-02-09T12:48:51.823043Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# import yolov5\n# display = yolov5.utils.notebook_init()  # check","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:48:51.826667Z","iopub.execute_input":"2022-02-09T12:48:51.826949Z","iopub.status.idle":"2022-02-09T12:48:51.831825Z","shell.execute_reply.started":"2022-02-09T12:48:51.826921Z","shell.execute_reply":"2022-02-09T12:48:51.830903Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# !pwd\n# !ls\n# !cat requirements.txt","metadata":{"execution":{"iopub.status.busy":"2022-02-09T12:48:51.833348Z","iopub.execute_input":"2022-02-09T12:48:51.833863Z","iopub.status.idle":"2022-02-09T12:48:51.844694Z","shell.execute_reply.started":"2022-02-09T12:48:51.833831Z","shell.execute_reply":"2022-02-09T12:48:51.843514Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"# üöÖ Training","metadata":{}},{"cell_type":"code","source":"!python train.py --img {DIM}\\\n--batch {BATCH}\\\n--epochs {EPOCHS}\\\n--optimizer {OPTMIZER}\\\n--data /kaggle/working/gbr.yaml\\\n--hyp /kaggle/working/hyp.yaml\\\n--weights {MODEL}.pt\\\n--project {PROJECT} --name {NAME}\\\n--exist-ok","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-09T07:45:25.937439Z","iopub.execute_input":"2022-02-09T07:45:25.937771Z","iopub.status.idle":"2022-02-09T09:07:55.747896Z","shell.execute_reply.started":"2022-02-09T07:45:25.937736Z","shell.execute_reply":"2022-02-09T09:07:55.746455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ‚ú® Overview\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\"><a href=\"https://wandb.ai/obengdouglas/great-barrier-reef-public\">View the Complete Dashboard Here ‚Æï</a></span>\n![image.png](https://github.com/denniesbor/KAGGLE-PROTECT-THE-GREAT-BARRIER-REEF/blob/assets/Screenshot%202022-02-09%20124249.png?raw=true)","metadata":{}},{"cell_type":"markdown","source":"## Output Files","metadata":{}},{"cell_type":"code","source":"OUTPUT_DIR = '{}/{}'.format(PROJECT, NAME)\n!ls {OUTPUT_DIR}","metadata":{"execution":{"iopub.status.busy":"2022-02-09T07:44:58.933684Z","iopub.execute_input":"2022-02-09T07:44:58.93401Z","iopub.status.idle":"2022-02-09T07:44:59.720328Z","shell.execute_reply.started":"2022-02-09T07:44:58.933973Z","shell.execute_reply":"2022-02-09T07:44:59.719175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìà Class Distribution","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10,10))\nplt.axis('off')\nplt.imshow(plt.imread(f'{OUTPUT_DIR}/labels_correlogram.jpg'));","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-09T07:44:59.722171Z","iopub.execute_input":"2022-02-09T07:44:59.723026Z","iopub.status.idle":"2022-02-09T07:44:59.880376Z","shell.execute_reply.started":"2022-02-09T07:44:59.722982Z","shell.execute_reply":"2022-02-09T07:44:59.878465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10,10))\nplt.axis('off')\nplt.imshow(plt.imread(f'{OUTPUT_DIR}/labels.jpg'));","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-09T07:44:59.881916Z","iopub.status.idle":"2022-02-09T07:44:59.883133Z","shell.execute_reply.started":"2022-02-09T07:44:59.882757Z","shell.execute_reply":"2022-02-09T07:44:59.882795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üî≠ Batch Image","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize = (10, 10))\nplt.imshow(plt.imread(f'{OUTPUT_DIR}/train_batch0.jpg'))\n\nplt.figure(figsize = (10, 10))\nplt.imshow(plt.imread(f'{OUTPUT_DIR}/train_batch1.jpg'))\n\nplt.figure(figsize = (10, 10))\nplt.imshow(plt.imread(f'{OUTPUT_DIR}/train_batch2.jpg'))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-09T07:44:59.884854Z","iopub.status.idle":"2022-02-09T07:44:59.886395Z","shell.execute_reply.started":"2022-02-09T07:44:59.88607Z","shell.execute_reply":"2022-02-09T07:44:59.886105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GT Vs Pred","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(3, 2, figsize = (2*9,3*5), constrained_layout = True)\nfor row in range(3):\n    ax[row][0].imshow(plt.imread(f'{OUTPUT_DIR}/val_batch{row}_labels.jpg'))\n    ax[row][0].set_xticks([])\n    ax[row][0].set_yticks([])\n    ax[row][0].set_title(f'{OUTPUT_DIR}/val_batch{row}_labels.jpg', fontsize = 12)\n    \n    ax[row][1].imshow(plt.imread(f'{OUTPUT_DIR}/val_batch{row}_pred.jpg'))\n    ax[row][1].set_xticks([])\n    ax[row][1].set_yticks([])\n    ax[row][1].set_title(f'{OUTPUT_DIR}/val_batch{row}_pred.jpg', fontsize = 12)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-09T07:44:59.888665Z","iopub.status.idle":"2022-02-09T07:44:59.890846Z","shell.execute_reply.started":"2022-02-09T07:44:59.890509Z","shell.execute_reply":"2022-02-09T07:44:59.890547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üîç Result","metadata":{}},{"cell_type":"markdown","source":"## Score Vs Epoch","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(30,15))\nplt.axis('off')\nplt.imshow(plt.imread(f'{OUTPUT_DIR}/results.png'));","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-09T07:44:59.893165Z","iopub.status.idle":"2022-02-09T07:44:59.895325Z","shell.execute_reply.started":"2022-02-09T07:44:59.894956Z","shell.execute_reply":"2022-02-09T07:44:59.894995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion Matrix","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.axis('off')\nplt.imshow(plt.imread(f'{OUTPUT_DIR}/confusion_matrix.png'));","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-09T07:44:59.89762Z","iopub.status.idle":"2022-02-09T07:44:59.899709Z","shell.execute_reply.started":"2022-02-09T07:44:59.899387Z","shell.execute_reply":"2022-02-09T07:44:59.89942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metrics","metadata":{}},{"cell_type":"code","source":"for metric in ['F1', 'PR', 'P', 'R']:\n    print(f'Metric: {metric}')\n    plt.figure(figsize=(12,10))\n    plt.axis('off')\n    plt.imshow(plt.imread(f'{OUTPUT_DIR}/{metric}_curve.png'));\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-09T07:44:59.901873Z","iopub.status.idle":"2022-02-09T07:44:59.904011Z","shell.execute_reply.started":"2022-02-09T07:44:59.903662Z","shell.execute_reply":"2022-02-09T07:44:59.903699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Please Upvote if you find this Helpful","metadata":{}},{"cell_type":"markdown","source":"# ‚úÇÔ∏è Remove Files","metadata":{}},{"cell_type":"code","source":"!rm -r {IMAGE_DIR}\n!rm -r {LABEL_DIR}","metadata":{"execution":{"iopub.status.busy":"2022-02-09T07:44:59.90635Z","iopub.status.idle":"2022-02-09T07:44:59.908459Z","shell.execute_reply.started":"2022-02-09T07:44:59.908133Z","shell.execute_reply":"2022-02-09T07:44:59.908168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Inference","metadata":{}},{"cell_type":"markdown","source":"# üõ† Install Libraries","metadata":{}},{"cell_type":"code","source":"# bbox-utility, check https://github.com/awsaf49/bbox for source code\n!pip install -q /kaggle/input/loguru-lib-ds/loguru-0.5.3-py3-none-any.whl\n!pip install -q /kaggle/input/bbox-lib-ds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìö Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport glob\nimport shutil\nimport sys\nsys.path.append('../input/tensorflow-great-barrier-reef')\nimport torch\nfrom PIL import Image","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìå Key-Points\n* One have to submit prediction using the provided **python time-series API**, which makes this competition different from previous Object Detection Competitions.\n* Each prediction row needs to include all bounding boxes for the image. Submission is format seems also **COCO** which means `[x_min, y_min, width, height]`\n* Copmetition metric `F2` tolerates some false positives(FP) in order to ensure very few starfish are missed. Which means tackling **false negatives(FN)** is more important than false positives(FP). \n$$F2 = 5 \\cdot \\frac{precision \\cdot recall}{4\\cdot precision + recall}$$","metadata":{}},{"cell_type":"code","source":"ROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef/'\n# CKPT_DIR  = '/kaggle/input/greatbarrierreef-yolov5-train-ds'\nCKPT_PATH = '/kaggle/input/reef-baseline-fold12/l6_3600_uflip_vm5_f12_up/f1/best.pt' # by @steamedsheep\nIMG_SIZE  = 9000\nCONF      = 0.25\nIOU       = 0.40\nAUGMENT   = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def predict(model, img, size=768, augment=False):\n    height, width = img.shape[:2]\n    results = model(img, size=size, augment=augment)  # custom inference size\n    preds   = results.pandas().xyxy[0]\n    bboxes  = preds[['xmin','ymin','xmax','ymax']].values\n    if len(bboxes):\n        bboxes  = voc2coco(bboxes,height,width).astype(int)\n        confs   = preds.confidence.values\n        return bboxes, confs\n    else:\n        return [],[]\n    \ndef format_prediction(bboxes, confs):\n    annot = ''\n    if len(bboxes)>0:\n        for idx in range(len(bboxes)):\n            xmin, ymin, w, h = bboxes[idx]\n            conf             = confs[idx]\n            annot += f'{conf} {xmin} {ymin} {w} {h}'\n            annot +=' '\n        annot = annot.strip(' ')\n    return annot\n\ndef show_img(img, bboxes, bbox_format='yolo'):\n    names  = ['starfish']*len(bboxes)\n    labels = [0]*len(bboxes)\n    img    = draw_bboxes(img = img,\n                           bboxes = bboxes, \n                           classes = names,\n                           class_ids = labels,\n                           class_name = True, \n                           colors = colors, \n                           bbox_format = bbox_format,\n                           line_thickness = 2)\n    return Image.fromarray(img).resize((800, 400))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Inference on **Train**","metadata":{}},{"cell_type":"code","source":"model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\nimage_paths = df[df.num_bbox>1].sample(100).image_path.tolist()\nfor idx, path in enumerate(image_paths):\n    img = cv2.imread(path)[...,::-1]\n    bboxes, confis = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n    display(show_img(img, bboxes, bbox_format='coco'))\n    if idx>5:\n        break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Init `Env`","metadata":{}},{"cell_type":"code","source":"import greatbarrierreef\nenv = greatbarrierreef.make_env()# initialize the environment\niter_test = env.iter_test()      # an iterator which loops over the test set and sample submission","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Inference on **Test**","metadata":{}},{"cell_type":"code","source":"model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\nfor idx, (img, pred_df) in enumerate(tqdm(iter_test)):\n    bboxes, confs  = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n    annot          = format_prediction(bboxes, confs)\n    pred_df['annotations'] = annot\n    env.predict(pred_df)\n    if idx<3:\n        display(show_img(img, bboxes, bbox_format='coco'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üëÄ Check Submission","metadata":{}},{"cell_type":"code","source":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","metadata":{},"execution_count":null,"outputs":[]}]}